# this will collect many papers that relates to self-supervied learning in vision domains.


Self-supervised learning for Images
|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|iGPT |	Generative Pretraining from Pixels |[paper](http://proceedings.mlr.press/v119/chen20s/chen20s.pdf) [code](https://github.com/openai/image-gpt) |__ICML 2021__|OpenAI|17 June 2020|
|2| MST | MST: Masked Self-Supervised Transformer for Visual Representation | [paper](https://arxiv.org/pdf/2106.05656.pdf) | __NeurIPS 2021__|Chinese Academy of Sciences| 10 June 2021|
|3|BEiT| BEiT: BERT Pre-Training of Image Transformers| [paper](https://arxiv.org/abs/2106.08254) [code](https://github.com/microsoft/unilm/tree/master/beit) | __ICLR 2022__|Microsoft Research| 15 June 2021|
|4| MAE | Masked Autoencoders Are Scalable Vision Learners| [paper](https://arxiv.org/pdf/2111.06377.pdf) [code](https://github.com/facebookresearch/mae)| CVPR 2022| Meta | 19 Dec 2021|
|5| iBoT | iBOT: Image BERT Pre-Training with Online Tokenizer| [paper](https://arxiv.org/pdf/2111.07832.pdf) [code](https://github.com/bytedance/ibot) | ICLR 2022 | ByteDance |15 Nov 2021| 
|6| SimMIM| SimMIM: A Simple Framework for Masked Image Modeling | [paper](https://arxiv.org/pdf/2111.09886.pdf) [code](https://github.com/microsoft/SimMIM) | arXiv| MSRA| 18 Nov 2021| 
|7| PeCo | 	PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers | [paper](https://arxiv.org/pdf/2111.12710.pdf) |arXiv|  Univeristy of Science and Technology of China | 24 Nov 2021|
|8| MaskFeat | 	Masked Feature Prediction for Self-Supervised Visual Pre-Training | [paper](https://arxiv.org/pdf/2112.09133.pdf) | arXiv | Meta | 16 Dec 2021|
|9| SplitMask | Are Large-scale Datasets Necessary for Self-Supervised Pre-training? | [paper](https://arxiv.org/pdf/2112.10740.pdf) | arXiv | Meta | 20 Dec 2021 | 
|10| ADIOS | Adversarial Masking for Self-Supervised Learning| [paper](https://arxiv.org/pdf/2201.13100.pdf) | ICML 2022 | Unviersity of Oxford | 31 Jan 2021|
|11| CAE | Context Autoencoder for Self-Supervised Representation Learning | [paper](https://arxiv.org/pdf/2202.03026.pdf) | arXiv | Peking University | 7 Feb 2022 |
|12| CIM| Corrupted Image Modeling for Self-Supervised Visual Pre-Training| [paper](https://arxiv.org/pdf/2202.03382.pdf) [code](https://github.com/microsoft/unilm) | arXiv | Microsoft | 7 Feb 2022|
|13| ConvMAE | ConvMAE: Masked Convolution Meets Masked Autoencoders |[paper](https://arxiv.org/pdf/2205.03892.pdf) [code](https://github.com/Alpha-VL/ConvMAE) | arXiv | Shanghai AI Laboratory |  19 May 2022 |
|14 | uniform masking | Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality | [paper](https://arxiv.org/pdf/2205.10063.pdf)  [code](https://github.com/implus/UM-MAE) | arXiv | Nanjing University of Science and Technology | 20 May 2022|
|15| LoMaR | Efficient self-supervised learning with local masked reconstruction | [paper](https://arxiv.org/pdf/2206.00790.pdf) [code](https://github.com/junchen14/LoMaR) | arXiv| KAUST | 1 Jun 2022 |
|16| M3AE | Multimodal Masked Autoencoders Learn Transferable Representations | [paper](https://arxiv.org/pdf/2205.14204.pdf) | arXiv | UCB | 31 May 2022|
|17| HiViT| HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling | [paper](https://arxiv.org/pdf/2205.14949.pdf) | arXiv | University of Chinese Academy of Sciences | 30 May 2022 |
|18 | GreenMiM |  Green Hierarchical Vision Transformer for Masked Image Modeling | [paper](https://arxiv.org/pdf/2205.13515v1.pdf) [code](https://github.com/LayneH/GreenMIM) | arXiv | The University of Tokyo | 26 May 2022 | 
|19| A^2MIM |Architecture-Agnostic Masked Image Modeling â€“ From ViT back to CNN   | [paper](https://arxiv.org/pdf/2205.13943.pdf) | arXiv | AI Lab, Westlake University | 1 Jun 2022|
|20 | MixMIM | MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning | [paper](https://arxiv.org/pdf/2205.13137.pdf) [code](https://github.com/Sense-X/MixMIM) | arXiv | SenseTime Research |  28 May 2022 |
|21 | SemMAE |SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders | [paper](https://arxiv.org/pdf/2206.10207.pdf) | arXiv | Chinese Academy of Sciences| 21 Jun 2022|
|22 | Voxel-MAE | Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds | [paper](https://arxiv.org/pdf/2206.09900.pdf) [code](https://github.com/chaytonmin/Voxel-MAE) | arXiv | Peking University | 20 Jun 2022|
|23 | BootMAE |Bootstrapped Masked Autoencoders for Vision BERT Pretraining| [paper](https://arxiv.org/pdf/2207.07116.pdf) [code](https://github.com/LightDXY/BootMAE) | ECCV 2022 | University of Science and Technology of China | 14 Jul 2022|
|24 | OmniMAE | OmniMAE: Single Model Masked Pretraining on Images and Videos| [paper](https://arxiv.org/pdf/2206.08356.pdf) [code](https://github.com/facebookresearch/omnivore) | arXiv | Meta AI | 16 Jun 2022|
|25 | SatMAE| SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery| [paper](https://arxiv.org/pdf/2207.08051.pdf) | arXiv | Stanford University | 17 Jul 2022 |
|26 | CMAE | Contrastive Masked Autoencoders are Stronger Vision Learners | [paper](https://arxiv.org/abs/2207.13532) | arXiv | University of Science and Technology | 27 Jul 2022 |
|27| BEiT v2 | BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | [paper](https://arxiv.org/pdf/2208.06366.pdf) | arXiv| University of Chinese Academy of Sciences | 12 Aug 2022|
|28| BEiT v3| Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks | [paper](https://arxiv.org/abs/2208.10442) | arXiv | Microsoft Corporation | 22 Aug 2022 |


Self-supervised Learning for Videos
|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1| VideoMAE| VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | [paper](https://arxiv.org/abs/2203.12602) [code](https://github.com/MCG-NJU/VideoMAE) | arXiv |  Tencent AI Lab | 23 Mar 2022 |
|2|MAE in Video| Masked Autoencoders As Spatiotemporal Learners | [paper](https://arxiv.org/pdf/2205.09113.pdf) | arXiv | Meta | 18 May 2022 |


Self-supervised Learning for Audios
|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1| AudioMAE| Masked Autoencoders that Listen | [paper](https://arxiv.org/pdf/2207.06405v1.pdf)  [code](https://github.com/facebookresearch/AudioMAE) | arXiv |  Meta AI | 13 Jul 2022|


Survey in self-supervised learning
|No.  |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:--------:|:---:|:-------:|
|1|A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond | [paper](https://arxiv.org/pdf/2208.00173.pdf) |arXiv| KAIST| 30 Jul 2022|

